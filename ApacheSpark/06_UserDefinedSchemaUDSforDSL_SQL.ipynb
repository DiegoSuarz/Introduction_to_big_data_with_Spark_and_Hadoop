{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Esquema Definido por el Usuario (UDS) para DSL y SQL\n",
                "**Tiempo estimado necesario:** 10 minutos\n",
                "**¿Cómo definir y aplicar un esquema definido por el usuario en PySpark?**\n",
                "En esta lectura, aprenderás cómo definir y aplicar un esquema definido por el usuario en PySpark.\n",
                "Spark proporciona un marco de procesamiento de datos estructurados que puede definir y aplicar esquemas para diversas fuentes de datos, incluidos los archivos CSV. Veamos los pasos para definir y usar un esquema definido por el usuario para un archivo CSV en PySpark:\n",
                "**Paso 1:**\n",
                "Importar las bibliotecas requeridas.\n",
                "\n",
                "```python\n",
                "from pyspark.sql.types import StructType, IntegerType, FloatType, StringType, StructField\n",
                "```\n",
                "\n",
                "**Paso 2:**\n",
                "Define el esquema.\n",
                "Entender los datos antes de definir un esquema es un paso importante.\n",
                "Veamos el enfoque paso a paso para entender los datos y definir un esquema apropiado para un archivo de entrada dado:\n",
                "1. **Explorar los datos:** Comprender los diferentes tipos de datos presentes en cada columna.\n",
                "    \n",
                "2. **Tipos de datos de las columnas:** Determinar los tipos de datos apropiados para cada columna según los valores observados.\n",
                "    \n",
                "3. **Definir el esquema:** Utiliza la clase 'StructType' en Spark y crea un 'StructField' para cada columna, mencionando el nombre de la columna, el tipo de dato y otras propiedades.\n",
                "    \n",
                "**Ejemplo:**\n",
                "\n",
                "```python\n",
                "schema = StructType([\n",
                "    StructField(\"Emp_Id\", StringType(), False),\n",
                "    StructField(\"Emp_Name\", StringType(), False),\n",
                "    StructField(\"Department\", StringType(), False),\n",
                "    StructField(\"Salary\", IntegerType(), False),\n",
                "    StructField(\"Phone\", IntegerType(), True),\n",
                "])\n",
                "```\n",
                "‘False’ indica que los valores nulos **NO** están permitidos para la columna.\n",
                "El esquema definido arriba se puede utilizar para los datos del archivo CSV a continuación:\n",
                "\n",
                "**Nombre del archivo: employee.csv**\n",
                "\n",
                "```python\n",
                "emp_id,emp_name,dept,salary,phone\n",
                "A101,jhon,computer science,1000,+1 (701) 846 958\n",
                "A102,Peter,Electronics,2000,\n",
                "A103,Micheal,IT,2500,\n",
                "```\n",
                "\n",
                "**Paso 3:** Lee el archivo de entrada con un esquema definido por el usuario.\n",
                "\n",
                "```python\n",
                "#create a dataframe on top a csv file\n",
                "df = (spark.read\n",
                "  .format(\"csv\")\n",
                "  .schema(schema)\n",
                "  .option(\"header\", \"true\")\n",
                "  .load(\"employee.csv\")\n",
                ")\n",
                "# display the dataframe content\n",
                "df.show()\n",
                "```\n",
                "\n",
                "**Paso 4:** Usa el `printSchema()` método en Spark para mostrar el esquema de un DataFrame y asegurar que el esquema se aplique correctamente a los datos.\n",
                "\n",
                "```python\n",
                "df.printSchema()\n",
                "```\n",
                "\n",
                "A través de los cuatro pasos anteriores, has adquirido la capacidad de establecer un esquema para un archivo CSV. Además, has utilizado este esquema definido por el usuario (UDF) para leer el archivo CSV, exhibir su contenido y mostrar el esquema en sí.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Installing required packages\n",
                "!pip install pyspark\n",
                "!pip install findspark\n",
                "!pip install pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "azdata_cell_guid": "9f29e6f9-8471-4c27-8074-1f7965ecfe96",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pyspark import SparkContext, SparkConf\n",
                "from pyspark.sql import SparkSession"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "azdata_cell_guid": "a4931c37-90ab-4e3e-9a5d-cd5ef42121b7",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "your 131072x1 screen size is bogus. expect trouble\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "25/12/02 18:22:02 WARN Utils: Your hostname, DiegoSrz resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
                        "25/12/02 18:22:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "25/12/02 18:22:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                        "25/12/02 18:22:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
                    ]
                }
            ],
            "source": [
                "# Crear la sesión de Spark\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"EmployeeCSV\") \\\n",
                "    .getOrCreate()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "azdata_cell_guid": "cb5d9285-ae9b-4185-a026-0f26b7bfab5e",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.types import StructType, IntegerType, FloatType, StringType, StructField\n",
                "\n",
                "schema = StructType([\n",
                "    StructField(\"Emp_Id\",StringType(), False),\n",
                "    StructField(\"Emp_name\",StringType(), False),\n",
                "    StructField(\"Department\",StringType(), False),\n",
                "    StructField(\"Salary\",IntegerType(),False),\n",
                "    StructField(\"Phone\",IntegerType(), True),\n",
                "])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "azdata_cell_guid": "5cf84903-1963-4dbf-a996-8f7e6deaf1a2",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "df = (\n",
                "    spark.read.format(\"csv\")\n",
                "    .schema(schema)\n",
                "    .option(\"header\", \"true\")\n",
                "    .load(\"employee.csv\")\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "azdata_cell_guid": "995d5a1d-eea3-436b-932e-cd5f092299ee",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "25/12/02 18:24:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
                        " Header: emp_id, emp_name, dept, salary, phone\n",
                        " Schema: Emp_Id, Emp_name, Department, Salary, Phone\n",
                        "Expected: Department but found: dept\n",
                        "CSV file: file:///mnt/c/Users/dg_su/Documents/Cursos/DATA_ENGINEER/Introduction_to_big_data_with_Spark_and_Hadoop/ApacheSpark/employee.csv\n",
                        "+------+--------+----------------+------+-----+\n",
                        "|Emp_Id|Emp_name|      Department|Salary|Phone|\n",
                        "+------+--------+----------------+------+-----+\n",
                        "|  A101|    jhon|computer science|  1000| null|\n",
                        "|  A102|   Peter|     Electronics|  2000| null|\n",
                        "|  A103| Micheal|              IT|  2500| null|\n",
                        "+------+--------+----------------+------+-----+\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "azdata_cell_guid": "8ccda23e-8ad5-4d61-87ad-62cfdac9b436",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- Emp_Id: string (nullable = true)\n",
                        " |-- Emp_name: string (nullable = true)\n",
                        " |-- Department: string (nullable = true)\n",
                        " |-- Salary: integer (nullable = true)\n",
                        " |-- Phone: integer (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.printSchema()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "spark-env (3.10.19)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

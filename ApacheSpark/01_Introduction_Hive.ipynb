{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Introducción a Hive\n",
                "\n",
                "En este laboratorio explorarás Apache Hive, un sistema de almacén de datos distribuido y tolerante a fallos que permite análisis a gran escala. Crearás una tabla y ejecutarás comandos SQL en ella.\n",
                "\n",
                "## Objetivos de Aprendizaje\n",
                "\n",
                "Al final de este laboratorio, serás capaz de:\n",
                "\n",
                "- Crear una tabla en Hive\n",
                "- Agregar datos a la tabla utilizando un archivo\n",
                "- Agregar datos a la tabla utilizando `insert`\n",
                "- Consultar los datos en la tabla utilizando comandos SQL\n",
                "\n",
                "## Requisitos Previos\n",
                "\n",
                "- Debes sentirte cómodo trabajando con la terminal de Linux\n",
                "- Tener conocimientos previos de SQL será útil\n",
                "\n",
                "> Aunque todos los comandos de terminal se pueden copiar y pegar para ejecutarlos, se recomienda encarecidamente que escribas los comandos para un mejor aprendizaje."
            ],
            "metadata": {
                "azdata_cell_guid": "37a920d2-08f6-4c69-a39e-1a2924fa80ee"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Paso 1: Obtén una copia del archivo CSV\n",
                "\n",
                "1. Ejecutarás los comandos en la terminal. Si no tienes una terminal abierta, abre una nueva terminal haciendo clic en `Terminal` y eligiendo `Nueva Terminal` en el submenú.\n",
                "\n",
                "    ![Menú de Terminal con Nueva Terminal resaltada.](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/New%20Terminal.png)\n",
                "\n",
                "2. Crea un directorio llamado `data` en `/home/project` ejecutando el siguiente comando.\n",
                "\n",
                "```\n",
                "mkdir /home/project/data\n",
                "```\n",
                "\n",
                "3. Cambia al directorio `/home/project/data`.\n",
                "\n",
                "```\n",
                "cd /home/project/data\n",
                "```\n",
                "\n",
                "4. Ejecuta el siguiente comando para obtener el `emp.csv`, un archivo de datos con información de empleados, en un archivo separado por comas que utilizarás más adelante para inyectar datos en la tabla que crearás.\n",
                "\n",
                "```\n",
                "wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/emp.csv\n",
                "```\n",
                "\n",
                "5. Abre el archivo en el editor y visualiza el archivo.\n",
                "\n",
                "    ![Panel del explorador con la carpeta de datos y Emp csv resaltada y abierta.](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/view_empcsv.png)"
            ],
            "metadata": {
                "azdata_cell_guid": "658c55b7-2ce5-4e50-b89a-749dcbe2f323"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Paso 2: Configurar Hive y Bee\n",
                "\n",
                "1. Utilizarás el hive del docker hub para este laboratorio. Descarga la imagen de hive en tu sistema ejecutando el siguiente comando.\n",
                "\n",
                "```\n",
                "docker pull apache/hive:4.0.0-alpha-1\n",
                "```\n",
                "\n",
                "> Esto tomará unos segundos, dependiendo de la velocidad de tu conexión a Internet.\n",
                "\n",
                "2. Ahora, ejecutarás el servidor hive en el puerto `10002`. Nombrarás la instancia del servidor `myhiveserver`. Montaremos la carpeta local `data` en el servidor hive como `hive_custom_data`. Esto significaría que toda la carpeta `data` que creaste localmente, junto con cualquier cosa que agregues en la carpeta de datos, se copia en el contenedor bajo el directorio `hive_custom_data`.\n",
                "\n",
                "```\n",
                "docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 -v /home/project/data:/hive_custom_data --name myhiveserver apache/hive:4.0.0-alpha-1\n",
                "```\n",
                "\n",
                "3. Puedes abrir y echar un vistazo al servidor Hive con la GUI. Haz clic en el botón para abrir la GUI de HiveServer2.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">HiveServer2 GUI</span>\n",
                "\n",
                "4. Ahora ejecuta el siguiente comando, que te permite acceder a `beeline`. Este es un CLI SQL donde puedes crear, modificar, eliminar tablas y acceder a los datos en la tabla.\n",
                "\n",
                "```\n",
                "docker exec -it myhiveserver beeline -u 'jdbc:hive2://localhost:10000/'\n",
                "```"
            ],
            "metadata": {
                "azdata_cell_guid": "2a60c279-1a60-4547-939f-283c82fca84e"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Paso 3: Crear tabla, agregar y ver datos\n",
                "\n",
                "1. Para crear una nueva tabla `Employee` con tres columnas como en el csv que descargaste - em\\_id, emp\\_name y salary, ejecuta el siguiente comando.\n",
                "\n",
                "```\n",
                "create table Employee(emp_id string, emp_name string, salary  int)  row format delimited fields terminated by ',' ;\n",
                "```\n",
                "\n",
                "> Puede notar que hay una mención explícita para los campos delimitados por `,` así como en el archivo csv.\n",
                "\n",
                "2. Ejecute el siguiente comando para verificar si la tabla ha sido creada.\n",
                "\n",
                "```\n",
                "show tables;\n",
                "```\n",
                "\n",
                "Esto debería listar la tabla de Empleados que acabas de crear.\n",
                "\n",
                "3. Ahora carga los datos en la tabla desde el archivo csv ejecutando el siguiente comando.\n",
                "\n",
                "```\n",
                "LOAD DATA INPATH '/hive_custom_data/emp.csv' INTO TABLE Employee;\n",
                "```\n",
                "\n",
                "3. Ejecuta el siguiente comando para listar todas las filas de la tabla y verificar si los datos se han cargado desde el CSV.\n",
                "\n",
                "```\n",
                "SELECT * FROM employee;\n",
                "```\n",
                "\n",
                "4. Puedes ver los detalles de los comandos y el resultado en la interfaz gráfica de HiveServer2.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">HiveServer2 GUI</span>\n",
                "\n",
                "5. Para salir del prompt de beehive en la terminal, presiona `ctrl+D`.\n",
                "\n",
                "Hive utiliza internamente MapReduce para procesar y analizar datos. Cuando ejecutas una consulta de Hive, genera trabajos de MapReduce que se ejecutan en el clúster de Hadoop."
            ],
            "metadata": {
                "azdata_cell_guid": "6f4db2ed-b0c4-4995-9466-07373fe57274"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Conclusión\n",
                "\n",
                "En este laboratorio creaste una tabla en hive, añadiste datos a la tabla desde un csv y listaste los datos contenidos en la tabla."
            ],
            "metadata": {
                "azdata_cell_guid": "8935b591-8ca6-45a2-9de9-8082dc3b9ded"
            },
            "attachments": {}
        }
    ]
}
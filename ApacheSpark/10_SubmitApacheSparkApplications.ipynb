{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "04d7f2a9-a431-4e6f-87fa-d6a00f64244b"
            },
            "source": [
                "# Enviar Aplicaciones de Apache Spark Lab\n",
                "\n",
                "En este laboratorio, aprenderás cómo enviar aplicaciones de Apache Spark desde un script de Python. Este ejercicio es sencillo, gracias a Docker Compose.\n",
                "\n",
                "## Objetivos de aprendizaje\n",
                "\n",
                "En este laboratorio, tú:\n",
                "\n",
                "- Instalarás un Master y Worker de Spark utilizando Docker Compose\n",
                "- Crearás un script de Python que contenga un trabajo de Spark\n",
                "- Enviarás el trabajo al clúster directamente desde Python (Nota: aprenderás cómo enviar un trabajo desde la línea de comandos en el laboratorio de Kubernetes)\n",
                "\n",
                "## Requisitos previos\n",
                "\n",
                "```\n",
                "**Nota**: Si estás ejecutando este laboratorio dentro del entorno del Laboratorio de Skills Network, todos los requisitos previos ya están instalados para ti\n",
                "\n",
                "```\n",
                "\n",
                "Los únicos requisitos previos para este laboratorio son:\n",
                "\n",
                "- La herramienta de línea de comandos _wget_\n",
                "- Un entorno de desarrollo de Python"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "72095245-a163-4858-85bf-be167321526e"
            },
            "source": [
                "# Iniciar el Maestro de Spark\n",
                "\n",
                "En el lado derecho de estas instrucciones, encontrarás el IDE en la nube. Selecciona la pestaña _Lab_. Luego, en la barra de menú, selecciona _Terminal_ \\> _Nuevo Terminal_.\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/NewTerminal.png)\n",
                "\n",
                "2. Ingresa los siguientes comandos en el terminal para descargar el entorno de Spark:\n",
                "\n",
                "```bash\n",
                "wget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz && tar xf spark-3.3.3-bin-hadoop3.tgz && rm -rf spark-3.3.3-bin-hadoop3.tgz\n",
                "\n",
                "```\n",
                "\n",
                "> Este proceso puede tardar un poco. Descarga Spark como un archivo comprimido y luego lo descomprime en el directorio actual.\n",
                "\n",
                "3. Ejecuta los siguientes comandos para configurar `JAVA_HOME` (preinstalado en el entorno) y `SPARK_HOME` (que acabas de descargar):\n",
                "\n",
                "```bash\n",
                "export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64export SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3\n",
                "\n",
                "```\n",
                "\n",
                "4. Ejecuta el siguiente comando para crear un archivo de configuración para el maestro:\n",
                "\n",
                "```bash\n",
                "touch /home/project/spark-3.3.3-bin-hadoop3/conf/spark-defaults.conf\n",
                "\n",
                "```\n",
                "\n",
                "5. Haga clic en el botón de abajo para comenzar el proceso de configuración del Spark Master.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">Open&nbsp;<strong style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px; margin: 0px; box-sizing: border-box;\">spark-defaults.conf</strong>&nbsp;in IDE</span>\n",
                "\n",
                "6. Pegue el siguiente contenido en el archivo `spark-defaults.conf`. Esto configurará el número de núcleos y la cantidad de memoria que el Master asignará a los trabajadores.\n",
                "\n",
                "```bash\n",
                "spark.executor.memory 4g\n",
                "spark.executor.cores 2\n",
                "\n",
                "```\n",
                "\n",
                "7. Navega al directorio `SPARK_HOME`:\n",
                "\n",
                "```bash\n",
                "cd $SPARK_HOME\n",
                "\n",
                "```\n",
                "\n",
                "8. Ejecuta el maestro de Spark ejecutando el siguiente comando:\n",
                "\n",
                "```bash\n",
                "./sbin/start-master.sh\n",
                "\n",
                "```\n",
                "\n",
                "9. Una vez que se inicie correctamente, haga clic en el botón de abajo para verificar que el Maestro está funcionando como se espera.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">Spark Master</span>\n",
                "\n",
                "Si la aplicación se ha iniciado correctamente, verá una página como la que se muestra a continuación.\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/yAkQovkEjJYMKrA-XyiqRQ/spark%20master.jpg)\n",
                "\n",
                "10. Copie la URL del Maestro como se muestra en la imagen y anótela en un editor de texto, como el Bloc de notas, en su computadora."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "7d90f5da-384f-4b7f-a161-6b6684fd2ab6"
            },
            "source": [
                "# Iniciar el trabajador\n",
                "\n",
                "1. Haz clic en `Terminal` en el menú y luego en `New Terminal` para abrir una nueva ventana de terminal.\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/NewTerminal.png)\n",
                "\n",
                "2. Una vez que la terminal se abra en la parte inferior de la ventana, ejecuta los siguientes comandos para configurar `JAVA_HOME` y `SPARK_HOME`:\n",
                "\n",
                "```bash\n",
                "export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\n",
                "export SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3\n",
                "```\n",
                "\n",
                "3. Navega al directorio `SPARK_HOME`:\n",
                "\n",
                "```bash\n",
                "cd $SPARK_HOME\n",
                "```\n",
                "\n",
                "4. Ejecuta el trabajador de Spark ejecutando el siguiente comando. Recuerda reemplazar el marcador `yourname` en el comando a continuación con tu nombre tal como aparece en la URL del maestro de Spark que anotaste en el paso anterior.\n",
                "\n",
                "```bash\n",
                "./sbin/start-worker.sh spark://theiadocker-yourname:7077 --cores 1 --memory 1g\n",
                "```\n",
                "\n",
                "5. Una vez que se inicie correctamente, haz clic en el botón de abajo para verificar que el Worker está funcionando como se espera.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">Spark Master</span>\n",
                "\n",
                "Deberías ver que el Worker ahora está registrado con el Master.\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/zYVs9FiWKEir2iinaXrEzw/spark-worker.jpg)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "fe2c4fb3-a2bb-4447-923a-c18ed7944f40"
            },
            "source": [
                "# Crear código y enviar\n",
                "\n",
                "1. Haz clic en `Terminal` en el menú y selecciona `Nuevo Terminal` para abrir una nueva ventana de terminal.\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/NewTerminal.png)\n",
                "\n",
                "2. Una vez que se abra la terminal en la parte inferior de la ventana, ejecuta el siguiente comando para crear el archivo de Python:\n",
                "\n",
                "```bash\n",
                "touch submit.py\n",
                "```\n",
                "\n",
                "Se crea un nuevo archivo de Python llamado `submit.py`.\n",
                "\n",
                "3. Abre el archivo en el editor de archivos haciendo clic en el botón de abajo o siguiendo la guía visual en la imagen.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">Open&nbsp;<strong style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px; margin: 0px; box-sizing: border-box;\">submit.py</strong>&nbsp;in IDE</span>\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/edit_submitpy.png)\n",
                "\n",
                "4. Pega el siguiente código en el archivo y guárdalo. Recuerda reemplazar el marcador `yourname` en el código a continuación con tu nombre tal como aparece en la URL del maestro de Spark.\n",
                "\n",
                "```python\n",
                "import findspark\n",
                "findspark.init()\n",
                "from pyspark import SparkContext, SparkConf\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                "    .master('spark://theiadocker-yourname:7077') \\\n",
                "    .config('spark.executor.cores', '1') \\\n",
                "    .config('spark.executor.memory', '512m') \\\n",
                "    .getOrCreate()\n",
                "\n",
                "df = spark.createDataFrame(\n",
                "    [\n",
                "        (1, \"foo\"),\n",
                "        (2, \"bar\"),\n",
                "    ],\n",
                "    StructType(\n",
                "        [\n",
                "            StructField(\"id\", IntegerType(), False),\n",
                "            StructField(\"txt\", StringType(), False),\n",
                "        ]\n",
                "    ),\n",
                ")\n",
                "print(df.dtypes)\n",
                "\n",
                "print(\"\\nDataFrame:\")\n",
                "df.show()\n",
                "```\n",
                "\n",
                "3. Ejecuta los siguientes comandos para configurar `JAVA_HOME` y `SPARK_HOME`:\n",
                "\n",
                "```bash\n",
                "export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\n",
                "export SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3\n",
                "```\n",
                "\n",
                "4. Instala los paquetes necesarios para configurar el entorno de Spark.\n",
                "\n",
                "```bash\n",
                "pip3 install findspark\n",
                "```\n",
                "\n",
                "5. Escribe el siguiente comando en la terminal para ejecutar el script de Python:\n",
                "\n",
                "```bash\n",
                "python3 submit.py\n",
                "```\n",
                "\n",
                "Verás la salida como a continuación:\n",
                "\n",
                "```bash\n",
                "Setting default log level to \"WARN\".\n",
                "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                "25/01/27 23:50:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                "[('id', 'int'), ('txt', 'string')]\n",
                "+---+---+                                                                       \n",
                "| id|txt|\n",
                "+---+---+\n",
                "|  1|foo|\n",
                "|  2|bar|\n",
                "+---+---+\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "487b15d6-9d31-4ee4-898e-983148503d82"
            },
            "source": [
                "# Experimenta tú mismo\n",
                "\n",
                "Por favor, echa un vistazo a la interfaz de usuario del Maestro y Trabajador de Apache Spark.\n",
                "\n",
                "1. Haz clic en el botón de abajo para lanzar el `Spark Master`. Alternativamente, haz clic en el botón de Skills Network a la izquierda. Esto abrirá la “Caja de herramientas de Skills Network.” Luego, haz clic en `Other` seguido de `Launch Application`. Desde allí, puedes ingresar el número de puerto como `8080` y lanzar la aplicación.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">Spark Master</span>\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Launch_Application--new_IDE.png)\n",
                "\n",
                "2. Esto te llevará a la interfaz de administración del Maestro de Spark (si tu bloqueador de ventanas emergentes no interfiere).\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nWC79kDOSPta7AdpAn24_A/submit-success.jpg)\n",
                "\n",
                "3. Ten en cuenta que puedes ver todos los trabajadores registrados (uno en este caso) y todos los trabajos en ejecución o completados (también uno en este caso).\n",
                "\n",
                "> Nota: Debido a las limitaciones del entorno del laboratorio, no puedes hacer clic en los enlaces dentro de la interfaz. En un entorno de producción típico, estos enlaces serían funcionales.\n",
                "\n",
                "4. Haz clic en el botón de abajo para abrir el `Spark Worker` en 8081. Alternativamente, haz clic en el botón de Skills Network a la izquierda, esto abrirá la “Caja de herramientas de Skills Network.” Luego, haz clic en `Other` seguido de `Launch Application`. Desde allí, deberías poder ingresar el número de puerto como `8081` y lanzar la aplicación.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">Spark Worker</span>\n",
                "\n",
                "Deberías poder encontrar tu trabajo en ejecución actualmente listado aquí."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "37723da4-490b-4b26-adbf-3d65dd783ae1"
            },
            "source": [
                "# Resumen\n",
                "\n",
                "En este laboratorio, has aprendido a configurar un clúster experimental de Apache Spark utilizando Docker Compose. Ahora puedes enviar un trabajo de Spark directamente desde código Python. En el siguiente laboratorio de Kubernetes, aprenderás a enviar trabajos de Spark desde la línea de comandos también."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "SQL",
            "language": "sql",
            "name": "SQL"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "d4e6b8e1-afb9-497a-88f8-bcc74fc6c3a8"
            },
            "source": [
                "# Monitoreo y Depuración de Apache Spark\n",
                "\n",
                "Este laboratorio te enseñará cómo monitorear y depurar una aplicación Spark a través de la interfaz web.\n",
                "\n",
                "## Objetivos\n",
                "\n",
                "Después de completar este laboratorio, podrás:\n",
                "\n",
                "1. Iniciar un clúster Spark en modo independiente y conectarte con el shell de PySpark.\n",
                "2. Crear un DataFrame y abrir la interfaz web de la aplicación.\n",
                "3. Depurar un error en tiempo de ejecución localizando la tarea fallida en la interfaz web.\n",
                "4. Ejecutar una consulta SQL para monitorear y luego escalar agregando otro trabajador al clúster."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "fcbde1b9-230a-4341-bebc-e21de67c8f24"
            },
            "source": [
                "# Ejercicio 1 : Iniciar un Clúster Standalone de Spark\n",
                "\n",
                "En este ejercicio, inicializarás un Clúster Standalone de Spark con un Maestro y un Trabajador. A continuación, iniciarás una consola de PySpark que se conecta al clúster y abrirás la interfaz web de la aplicación Spark para monitorearla. Estaremos utilizando el terminal Theia para ejecutar comandos y contenedores basados en docker para lanzar los procesos de Spark.\n",
                "\n",
                "### Tarea A : Descargar Datos de Ejemplo para Spark\n",
                "\n",
                "1. Abre un terminal Theia haciendo clic en el elemento del menú `Terminal -> New Terminal`.\n",
                "    \n",
                "2. Utiliza el siguiente comando para descargar el conjunto de datos que utilizaremos en este laboratorio al contenedor que ejecuta Spark.\n",
                "    \n",
                "\n",
                "```bash\n",
                "wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/cars.csv\n",
                "\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "4206db0c-907c-46f6-97be-e099c5c7788c"
            },
            "source": [
                "# Tarea B : Inicializar el Clúster\n",
                "\n",
                "1. Detén cualquier contenedor que esté corriendo previamente con el comando:\n",
                "\n",
                "```bash\n",
                "for i in `docker ps | awk '{print $1}' | grep -v CONTAINER`; do docker kill $i; done\n",
                "```\n",
                "\n",
                "2. Eliminar cualquier contenedor previamente utilizado:  \n",
                "      \n",
                "    Ignorar cualquier error que diga “No existe tal contenedor”\n",
                "\n",
                "```bash\n",
                "docker rm spark-master spark-worker-1 spark-worker-2\n",
                "```\n",
                "\n",
                "3. Inicia el servidor Master de Spark:\n",
                "\n",
                "```bash\n",
                "docker run \\\n",
                "    --name spark-master \\\n",
                "    -h spark-master \\\n",
                "    -e ENABLE_INIT_DAEMON=false \\\n",
                "    -p 4040:4040 \\\n",
                "    -p 8080:8080 \\\n",
                "    -v `pwd`:/home/root \\\n",
                "    -d bde2020/spark-master:3.1.1-hadoop3.2\n",
                "```\n",
                "\n",
                "4. Iniciar un trabajador de Spark que se conectará al Maestro:\n",
                "\n",
                "```bash\n",
                "docker run \\\n",
                "    --name spark-worker-1 \\\n",
                "    --link spark-master:spark-master \\\n",
                "    -e ENABLE_INIT_DAEMON=false \\\n",
                "    -p 8081:8081 \\\n",
                "    -v `pwd`:/home/root \\\n",
                "    -d bde2020/spark-worker:3.1.1-hadoop3.2\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "d4b3ca94-d932-4301-ac38-0b0b4787719a"
            },
            "source": [
                "# Tarea C : Conectar un Shell de PySpark al Clúster y Abrir la Interfaz de Usuario\n",
                "\n",
                "1. Inicie un shell de PySpark en el contenedor del Maestro de Spark en ejecución:\n",
                "\n",
                "```bash\n",
                "docker exec \\\n",
                "    -it `docker ps | grep spark-master | awk '{print $1}'` \\\n",
                "    /spark/bin/pyspark \\\n",
                "    --master spark://spark-master:7077\n",
                "```\n",
                "\n",
                "2. Crea un DataFrame en la terminal con:\n",
                "    \n",
                "    > **NOTA:** Presiona Enter dos veces para continuar después de ejecutar el comando en la terminal.\n",
                "    \n",
                "\n",
                "```bash\n",
                "df = spark.read.csv(\"/home/root/cars.csv\", header=True, inferSchema=True) \\\n",
                "    .repartition(32) \\\n",
                "    .cache()\n",
                "df.show()\n",
                "```\n",
                "\n",
                "3. Haz clic en el botón Skills Network a la izquierda, se abrirá la “Caja de herramientas de Skills Network”. Luego haz clic en `OTHER` y después en `Launch Application`. Desde allí deberías poder ingresar el número de puerto como `4040` y lanzar la interfaz de usuario de la aplicación Spark en tu navegador.\n",
                "\n",
                "    ![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Launch_Application--new_IDE.png)\n",
                "\n",
                "  \n",
                "  \n",
                "\n",
                "4. Verifica que puedes ver la página de trabajos de la aplicación que debería verse como la siguiente, aunque no necesariamente exactamente igual:\n",
                "\n",
                "    ![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/SparkUI-Initial-Page.png)\n",
                "    "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "b91202ce-7ef7-4379-ab7b-90cf3a4dd797"
            },
            "source": [
                "# Ejercicio 2 : Ejecutar una consulta SQL y depurar en la interfaz de la aplicación\n",
                "\n",
                "En este ejercicio, definirás una función definida por el usuario (UDF) y ejecutarás una consulta que resulta en un error. Localizaremos ese error en la interfaz de la aplicación y encontraremos la causa raíz. Finalmente, corregiremos el error y volveremos a ejecutar la consulta.\n",
                "\n",
                "### Tarea A : Ejecutar una consulta SQL\n",
                "\n",
                "1. Define una UDF para mostrar el tipo de motor. Copia y pega el código y haz clic en `Enter`.\n",
                "\n",
                "```python\n",
                "from pyspark.sql.functions import udf\n",
                "import time\n",
                "\n",
                "@udf(\"string\")\n",
                "def engine(cylinders):\n",
                "    time.sleep(0.2)  # Intentionally delay task\n",
                "    eng = {6: \"V6\", 8: \"V8\"}\n",
                "    return eng[cylinders]\n",
                "```\n",
                "\n",
                "2. Agrega la UDF como una columna en el DataFrame\n",
                "\n",
                "```\n",
                "df = df.withColumn(\"engine\", engine(\"cylinders\"))\n",
                "```\n",
                "\n",
                "3. Agrupar el DataFrame por “cylinders” y agregar otras columnas\n",
                "\n",
                "```python\n",
                "dfg = df.groupby(\"cylinders\")\n",
                "```\n",
                "```python\n",
                "dfa = dfg.agg({\"mpg\": \"avg\", \"engine\": \"first\"})\n",
                "```\n",
                "```python\n",
                "dfa.show()\n",
                "```\n",
                "\n",
                "4. La consulta habrá fallado y deberías ver muchos mensajes y salidas en la consola. La siguiente tarea será localizar el error en la interfaz de usuario de la aplicación y determinar la causa raíz."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1d4cbf53-6bd6-4c60-beb3-469dc2b91f63"
            },
            "source": [
                "# Tarea B : Depurar el error en la interfaz de la aplicación\n",
                "\n",
                "1. Encuentra el error en la interfaz de la aplicaciónAbre la interfaz de usuario en los trabajos, mira la lista de trabajos fallidos, haz clic en el primer trabajo.\n",
                "\n",
                "    ![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Failure-Jobs-tab.png)\n",
                "\n",
                "2. Esto mostrará los detalles del trabajo  \n",
                "    con una lista de etapas para ese trabajo. En la lista de etapas fallidas, haz clic en la primera  \n",
                "    etapa fallida para mostrar los detalles de la etapa con una lista de tareas para esa etapa.\n",
                "\n",
                "    ![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Failure-Jobs-detail.png)\n",
                "\n",
                "3. Aquí vemos muchastareas fallidas. Al mirar la primera, la columna de la derecha muestra detalles de la falla.\n",
                "\n",
                "    ![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Failure-Stage-list.png)\n",
                "\n",
                "Haz clic para expandir los detalles.\n",
                "\n",
                "![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Failure-Task-details.png)\n",
                "\n",
                "Desplázate hacia abajo un poco hasta que puedas ver la última parte del error de Python que muestra la causa. Deberías poder ver que esto fue causado por un KeyError en nuestra UDF  \n",
                "`engine()`.\n",
                "\n",
                "También podrías ver estos errores mirando la columna que tiene enlaces a los registros y hacer clic en  \n",
                "“std err” para mostrar el registro de errores estándar.\n",
                "\n",
                "Cierra la pestaña del navegador de PySpark.\n",
                "\n",
                "4. En la terminal de Theia, corrige la UDF agregando una entrada al diccionario de tipos de motor y proporciona un valor predeterminado para todos los demás tipos. Copia y pega este código y presiona `Enter`.\n",
                "\n",
                "```python\n",
                "@udf(\"string\")\n",
                "def engine(cylinders):\n",
                "    time.sleep(0.2)  # Retrasar intencionadamente la tarea\n",
                "    eng = {4: \"inline-four\", 6: \"V6\", 8: \"V8\"}\n",
                "    return eng.get(cylinders, \"other\")\n",
                "```\n",
                "\n",
                "5. Vuelve a ejecutar la consulta. Tendrás que agregar la columna “engine” nuevamente e ingresar la consulta ya que cambiamos la UDF.\n",
                "\n",
                "```python\n",
                "df = df.withColumn(\"engine\", engine(\"cylinders\"))\n",
                "```\n",
                "```python\n",
                "dfg = df.groupby(\"cylinders\")\n",
                "```\n",
                "```python\n",
                "dfa = dfg.agg({\"mpg\": \"avg\", \"engine\": \"first\"})\n",
                "```\n",
                "```python\n",
                "dfa.show()\n",
                "```\n",
                "\n",
                "Once the query completes without errors, you should see output similar to this.\n",
                "\n",
                "```bash\n",
                "+---------+------------------+-------------+                                    \n",
                "|cylinders|          avg(mpg)|first(engine)|\n",
                "+---------+------------------+-------------+\n",
                "|        6|19.985714285714288|           V6|\n",
                "|        3|             20.55|        other|\n",
                "|        5|27.366666666666664|        other|\n",
                "|        4|29.286764705882348|  inline-four|\n",
                "|        8|14.963106796116506|           V8|\n",
                "+---------+------------------+-------------+\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "387fa8c8-f648-4eb2-bda6-c2355a738db8"
            },
            "source": [
                "# Ejercicio 3 : Monitorear el Rendimiento de la Aplicación con la UI\n",
                "\n",
                "Ahora que hemos ejecutado nuestra consulta con éxito, escalaremos nuestra aplicación añadiendo un trabajador al clúster. Esto permitirá que el clúster ejecute más tareas en paralelo y mejore el rendimiento general.\n",
                "\n",
                "### Tarea A : Agregar un Trabajador al Clúster\n",
                "\n",
                "1. Ve a la pestaña de Etapas, luego haz clic en la etapa con 32 tareas. En esa etapa, nuestra UDF se está aplicando a cada partición del DataFrame.\n",
                "\n",
                "![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Perf-stage-udf.png)\n",
                "\n",
                "Al observar la línea de tiempo, puedes ver que hay un único trabajador con id `0 / <ip-address>` que puede ejecutar hasta una cierta cantidad de tareas en paralelo al mismo tiempo. Agregar otro trabajador permitirá que se ejecute una tarea adicional en paralelo.\n",
                "\n",
                "![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Perf-parallel-tasks-1.png)\n",
                "\n",
                "2. Abre un nuevo terminal de Theia haciendo clic en el elemento del menú `Terminal -> New Terminal`.\n",
                "    \n",
                "3. Agrega un segundo trabajador al clúster con el comando en el nuevo terminal:\n",
                "    \n",
                "\n",
                "```bash\n",
                "docker run \\\n",
                "    --name spark-worker-2 \\\n",
                "    --link spark-master:spark-master \\\n",
                "    -e ENABLE_INIT_DAEMON=false \\\n",
                "    -p 8082:8082 \\\n",
                "    -d bde2020/spark-worker:3.1.1-hadoop3.2\n",
                "```\n",
                "\n",
                "4. Si el comando es exitoso, habrá una única salida que mostrará el id del contenedor:\n",
                "\n",
                "```bash\n",
                "theia@theiadocker-user:/home/project$ docker run \\\n",
                ">     --name spark-worker-2 \\\n",
                ">     --link spark-master:spark-master \\\n",
                ">     -e ENABLE_INIT_DAEMON=false \\\n",
                ">     -p 8082:8082 \\\n",
                ">     -d bde2020/spark-worker:3.1.1-hadoop3.2\n",
                "1935a71827668ae3476e6a16f0bebcd4c2a342a21271dc22be487aa1b1731708\n",
                "theia@theiadocker-user:/home/project$\n",
                "```\n",
                "\n",
                "5. Haz clic de nuevo en la primera terminal que tiene el shell de PySpark abierto para continuar."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "8b333f33-5c18-4997-b81b-a82104b18b2d"
            },
            "source": [
                "# Tarea B : Volver a ejecutar la consulta y verificar el rendimiento\n",
                "\n",
                "1. Vuelve a ejecutar la consulta, esta vez simplemente podemos llamar a `show()` nuevamente:\n",
                "\n",
                "```python\n",
                "dfa.show()\n",
                "```\n",
                "\n",
                "2. Inicie la aplicación en el número de puerto `4040` siguiendo el mismo proceso que arriba, para abrir el navegador de PySpark.  \n",
                "    Vaya a la pestaña **Stages** y vea el Id de la etapa más reciente. \n",
                "     \n",
                "    ![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/RecentStageView.png)\n",
                "    \n",
                "3. Verá que el trabajador adicional con id `1 / <ip-address>` está listado y ahora permite que se ejecuten más tareas en paralelo. La línea de tiempo de las tareas debería parecerse a lo siguiente.\n",
                "    \n",
                "\n",
                "    ![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Perf-parallel-tasks-2.png)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "SQL",
            "language": "sql",
            "name": "SQL"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

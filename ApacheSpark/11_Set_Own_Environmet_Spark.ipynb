{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "7dec16fe-6d79-43c2-9abf-5f29ada24f04"
            },
            "source": [
                "# Lectura: Cómo configurar tus propios entornos de Spark\n",
                "\n",
                "**Tiempo estimado necesario:** 5 minutos\n",
                "\n",
                "Después de completar esta lectura, podrás configurar y usar Apache Spark en tu computadora. Luego crearás un entorno para desarrollar y probar aplicaciones de Spark.\n",
                "\n",
                "Spark es una herramienta poderosa que te permite trabajar con grandes conjuntos de datos en múltiples computadoras simultáneamente.\n",
                "\n",
                "Aquí tienes una guía simple para ayudarte a comenzar:\n",
                "\n",
                "**1\\. Requisitos previos:**\n",
                "\n",
                "- **Java**: Spark está construido sobre Java, por lo que necesitarás tener Java instalado. Spark requiere Java 8 o versiones posteriores.\n",
                "\n",
                "Por favor abre la terminal (en Mac) o el símbolo del sistema (en Windows), escribe `java -version`, y presiona return o enter.\n",
                "\n",
                "![Cómo configurar tu propio entorno de Spark.](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Spark%20Picture1.png)\n",
                "\n",
                "- **Python (opcional)**: Aunque Spark está escrito principalmente en Scala, proporciona APIs para múltiples lenguajes, incluyendo Python. Puedes usar Scala, Java, Python o R para trabajar con Spark.\n",
                "    \n",
                "- **Hadoop (opcional)**: Spark puede ejecutarse sobre el Sistema de Archivos Distribuido de Hadoop (HDFS), pero no necesitas necesariamente Hadoop para el desarrollo local.\n",
                "    \n",
                "\n",
                "**2\\. Descargar Spark:**\n",
                "\n",
                "Ve al sitio web oficial de Spark ([https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)) y descarga la última versión de Spark. Elige el paquete preconstruido para Hadoop con la versión adecuada de Spark, Scala y Hadoop.\n",
                "\n",
                "![Cómo configurar tu propio entorno de Spark.](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Picture2nd.png)\n",
                "\n",
                "**3\\. Configurar variables de entorno:**\n",
                "\n",
                "Necesitas configurar un par de variables de entorno para que Spark funcione correctamente:\n",
                "\n",
                "- **SPARK\\_HOME**: Apunta esta variable al directorio donde extrajiste Spark.\n",
                "    \n",
                "- **PATH**: Agrega `%SPARK_HOME%\\bin` a tu PATH para acceder fácilmente a los comandos de Spark.\n",
                "    \n",
                "\n",
                "**4\\. Configuración (Opcional):**\n",
                "\n",
                "En el directorio `conf` dentro de tu instalación de Spark, encontrarás varios archivos de configuración. El más importante es `spark-defaults.conf`, donde puedes establecer propiedades de Spark. Sin embargo, para el desarrollo local, las configuraciones predeterminadas suelen ser suficientes.\n",
                "\n",
                "**5\\. Iniciar Spark:**\n",
                "\n",
                "**Shell interactivo (Scala o Python)**: Puedes iniciar el shell interactivo de Spark utilizando los siguientes comandos:\n",
                "\n",
                "- **Scala**: Ejecuta `spark-shell` en tu terminal.\n",
                "    \n",
                "- **Python**: Ejecuta `pyspark` en tu terminal.\n",
                "    \n",
                "\n",
                "![Cómo configurar tu propio entorno de Spark.](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Picture3.png)\n",
                "\n",
                "**Envío de aplicaciones:** Puedes enviar aplicaciones de Spark de manera similar:\n",
                "\n",
                "- **Scala**: `spark-submit --class <main-class> --master local <path-to-jar>`\n",
                "    \n",
                "- **Python**: `spark-submit --master local <path-to-python-script>`\n",
                "    \n",
                "\n",
                "**6\\. Escribir aplicaciones de Spark:**\n",
                "\n",
                "Las aplicaciones de Spark se escriben típicamente utilizando las APIs de Spark. Puedes usar Conjuntos de Datos Distribuidos Resilientes (RDDs) o DataFrames y conjuntos de datos para operaciones más estructuradas y optimizadas.\n",
                "\n",
                "Aquí tienes un ejemplo simple utilizando Python y DataFrames.\n",
                "\n",
                "**Archivo de entrada:** `data.csv`\n",
                "\n",
                "Datos de muestra:\n",
                "\n",
                "| Nombre | Puntaje |\n",
                "| --- | --- |\n",
                "| A | 10 |\n",
                "| B | 15 |\n",
                "| A | 20 |\n",
                "| B | 5 |\n",
                "| A | 30 |\n",
                "\n",
                "```python\n",
                "from pyspark.sql import SparkSession \n",
                "\n",
                "# Crear una sesión de Spark \n",
                "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
                "\n",
                "# Cargar datos\n",
                "data = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
                "data.show()\n",
                "\n",
                "# Realizar operaciones\n",
                "result = data.groupBy(\"name\").agg({\"score\": \"sum\"})\n",
                "\n",
                "# Mostrar resultado \n",
                "result.show() \n",
                "\n",
                "# Detener la sesión de Spark\n",
                "spark.stop() \n",
                "```\n",
                "\n",
                "**7\\. Monitoreo:**\n",
                "\n",
                "Spark proporciona una interfaz web (por defecto en [http://localhost:4040](http://localhost:4040/)) para monitorear tus aplicaciones de Spark y su progreso.\n",
                "\n",
                "**8\\. Limpieza:**\n",
                "\n",
                "Asegúrate de detener la sesión de Spark después de haber liberado los recursos.\n",
                "\n",
                "```python\n",
                "spark.stop() \n",
                "```\n",
                "\n",
                "Esta es una guía básica para comenzar con Spark en tus propias máquinas. Para configuraciones y optimizaciones más avanzadas, puedes consultar la documentación oficial de Spark: [https://spark.apache.org/documentation.html](https://spark.apache.org/documentation.html)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "your 131072x1 screen size is bogus. expect trouble\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "25/12/02 22:43:13 WARN Utils: Your hostname, DiegoSrz resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
                        "25/12/02 22:43:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "25/12/02 22:43:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                        "25/12/02 22:43:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
                        "25/12/02 22:43:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
                        "25/12/02 22:43:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+----+-----+\n",
                        "|name|score|\n",
                        "+----+-----+\n",
                        "|   A|   10|\n",
                        "|   B|   15|\n",
                        "|   A|   20|\n",
                        "|   B|    5|\n",
                        "|   A|   30|\n",
                        "+----+-----+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
                "\n",
                "data = spark.read.csv(\"data.csv\",header=True, inferSchema=True)\n",
                "data.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+----+----------+\n",
                        "|name|sum(score)|\n",
                        "+----+----------+\n",
                        "|   B|        20|\n",
                        "|   A|        60|\n",
                        "+----+----------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "result = data.groupBy(\"name\").agg({\"score\":\"sum\"})\n",
                "result.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "spark-env (3.10.19)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e4a033f2-3dc8-4e8c-a59e-3ab32ded8da4"
            },
            "source": [
                "# Métodos de ingestión de datos por lotes y manejo de datos de diversas fuentes\n",
                "\n",
                "En esta lectura, descubrirás varios métodos de ingestión de datos por lotes en Apache Spark. También aprenderás a manejar datos de diversas fuentes.\n",
                "\n",
                "Comencemos con los métodos de ingestión de datos por lotes.\n",
                "\n",
                "### **1\\. Ingesta basada en archivos**\n",
                "\n",
                "**¿Qué es?**  \n",
                "La ingesta basada en archivos se refiere al proceso de importar datos desde varios formatos de archivo a Apache Spark.\n",
                "\n",
                "**¿Por qué es importante?**  \n",
                "Entender la ingesta basada en archivos es crucial cuando trabajas con conjuntos de datos diversos almacenados en diferentes formatos.\n",
                "\n",
                "**Técnicas clave:  \n",
                "• Ingesta de CSV:**\n",
                "\n",
                "```python\n",
                "df_csv = spark.read.csv(\"file.csv\", header=True)df_csv.show()\n",
                "\n",
                "```\n",
                "\n",
                "**• Ingesta de JSON:**\n",
                "\n",
                "```python\n",
                "df_json = spark.read.json(\"file.json\")df_json.show()\n",
                "\n",
                "```\n",
                "\n",
                "**• Ingesta de Parquet:**\n",
                "\n",
                "```python\n",
                "df_parquet = spark.read.parquet(\"file.parquet\")df_parquet.show()\n",
                "\n",
                "```\n",
                "\n",
                "**• Ingesta de cXML:**  \n",
                "_Nota: El soporte de XML puede requerir bibliotecas adicionales_\n",
                "\n",
                "```python\n",
                "df_xml = spark.read.format(\"com.databricks.spark.xml\").option(\"rowTag\", \"record\").load(\"file.xml\")\n",
                "df_xml.show()\n",
                "\n",
                "```\n",
                "\n",
                "### **2\\. Replicación de bases de datos y extracción, transformación y carga (ETL)**\n",
                "\n",
                "**¿Por qué es importante?**  \n",
                "Puedes aprovechar la replicación de bases de datos y los procesos ETL para facilitar la ingesta y transformación continua de datos.\n",
                "\n",
                "**Perspectivas clave:**  \n",
                "• **Replicación de bases de datos**\n",
                "\n",
                "```python\n",
                "# Assuming df_db is the DataFrame from the replicated database\n",
                "df_db.write.mode(\"append\").saveAsTable(\"database_table\")\n",
                "\n",
                "```\n",
                "\n",
                "• **Proceso ETL:**\n",
                "\n",
                "```python\n",
                "# Assuming df_raw is the raw data DataFrame\n",
                "df_transformed = df_raw.select(\"col1\", \"col2\").withColumn(\"new_col\", expr(\"col1 + col2\"))\n",
                "df_transformed.write.mode(\"append\").saveAsTable(\"transformed_table\")\n",
                "\n",
                "```\n",
                "\n",
                "### 3\\. Importación de datos a través de interfaces de programación de aplicaciones (APIs)\n",
                "\n",
                "**¿Por qué es importante?**  \n",
                "Integrar APIs externas sin problemas te permite recuperar e ingerir datos de manera eficiente.\n",
                "\n",
                "**Consideraciones clave:  \n",
                "• Integración de API HTTP:**\n",
                "\n",
                "```python\n",
                "import requests\n",
                "response = requests.get(\"https://api.example.com/data\")\n",
                "json_data = response.json()\n",
                "df_api = spark.read.json(spark.sparkContext.parallelize([json_data]))\n",
                "df_api.show()\n",
                "\n",
                "```\n",
                "\n",
                "### **4\\. Protocolos de transferencia de datos**\n",
                "\n",
                "**¿Qué son?**  \n",
                "Los protocolos de transferencia de datos como el Protocolo de Transferencia de Archivos (FTP), el Protocolo de Transferencia de Archivos Seguro (SFTP) y el Protocolo de Transferencia de Hipertexto (HTTP) son esenciales para una transferencia de datos eficiente y segura.\n",
                "\n",
                "**Mejores Prácticas:  \n",
                "• Ingesta FTP:**\n",
                "\n",
                "```python\n",
                "spark.read.format(\"com.springml.spark.sftp\").option(\"host\", \"ftp.example.com\").load(\"/path/to/file.csv\")\n",
                "\n",
                "```\n",
                "\n",
                "**• Ingesta HTTP:**\n",
                "\n",
                "```python\n",
                "spark.read.text(\"http://example.com/data.txt\")\n",
                "\n",
                "```\n",
                "\n",
                "Ahora, veamos varias formas en las que puedes manejar datos de diversas fuentes.\n",
                "\n",
                "### 1\\. Evaluación de la fuente de datos\n",
                "\n",
                "**¿Por qué es importante?**  \n",
                "Debes evaluar las características y la calidad de la fuente de datos, ya que son fundamentales para una integración de datos efectiva.\n",
                "\n",
                "**Actividades clave:  \n",
                "• Evaluación de características:**\n",
                "\n",
                "```python\n",
                "df_source.describe().show()\n",
                "\n",
                "```\n",
                "\n",
                "**• Evaluación de calidad:**\n",
                "\n",
                "```python\n",
                "df_source.selectExpr(\"count(distinct *) as unique_records\").show()\n",
                "\n",
                "```\n",
                "\n",
                "### 2\\. Mapeo y transformación de esquemas\n",
                "\n",
                "**¿Cuáles son los desafíos?**  \n",
                "Mapear y transformar esquemas de datos puede ser un desafío para ti, pero al mismo tiempo, son esenciales para adaptar los datos al modelo objetivo.\n",
                "\n",
                "**Técnicas clave:  \n",
                "• Mapeo de esquemas:**\n",
                "\n",
                "```python\n",
                "df_mapped = df_raw.selectExpr(\"col1 as new_col1\", \"col2 as new_col2\")\n",
                "\n",
                "```\n",
                "\n",
                "**• Transformación de esquema:**\n",
                "\n",
                "```python\n",
                "df_transformed = df_mapped.withColumn(\"new_col3\", expr(\"new_col1 + new_col2\"))\n",
                "\n",
                "```\n",
                "\n",
                "### 3\\. Validación y limpieza de datos\n",
                "\n",
                "**¿Por qué es importante?**  \n",
                "Debes garantizar una alta calidad de los datos durante la ingestión, ya que es crucial para los procesos posteriores.\n",
                "\n",
                "**Estrategias clave:  \n",
                "• Validación de datos:**\n",
                "\n",
                "```python\n",
                "df_validated = df_raw.filter(\"col1 IS NOT NULL AND col2 > 0\")\n",
                "\n",
                "```\n",
                "\n",
                "**• Limpieza de datos:**\n",
                "\n",
                "```python\n",
                "df_cleansed = df_raw.na.fill(0, [\"col1\"])\n",
                "\n",
                "```\n",
                "\n",
                "### 4\\. Deducción de datos\n",
                "\n",
                "**¿Por qué es crucial?**  \n",
                "Debes prevenir registros duplicados durante la ingestión, ya que es vital mantener la integridad de los datos en el futuro.\n",
                "\n",
                "**Estrategias clave:  \n",
                "• Eliminar duplicados:**\n",
                "\n",
                "```python\n",
                "df_deduplicated = df_raw.dropDuplicates([\"col1\", \"col2\"])\n",
                "\n",
                "```\n",
                "\n",
                "### **5\\. Manejo de datos no estructurados**\n",
                "\n",
                "**¿Qué son los datos no estructurados?**  \n",
                "Los datos no estructurados incluyen documentos, imágenes, registros y más. Las técnicas para la ingestión y extracción de conocimientos son cruciales.\n",
                "\n",
                "**Técnicas avanzadas:  \n",
                "• Procesamiento de Lenguaje Natural (NLP):**\n",
                "\n",
                "```python\n",
                "# Using Spark NLP library for text data processing\n",
                "df_text = spark.read.text(\"text_file.txt\")\n",
                "\n",
                "```\n",
                "\n",
                "### 6\\. Gobernanza de datos y cumplimiento\n",
                "\n",
                "**¿Por qué es importante?**  \n",
                "Debes asegurar prácticas de gobernanza de datos y cumplimiento con los requisitos regulatorios y las leyes de privacidad de datos, ya que son esenciales para un manejo responsable de los datos.\n",
                "\n",
                "**Prácticas clave:  \n",
                "• Controles de cumplimiento:**\n",
                "\n",
                "```python\n",
                "# Example: Ensure GDPR compliancedf_gd\n",
                "pr_compliant = df_raw.filter(\"country IN ('EU')\")\n",
                "\n",
                "```\n",
                "\n",
                "Esta lectura sirve como una guía completa para navegar a través de los métodos de ingestión de datos por lotes en Spark. Puedes profundizar en los temas que se alineen con tus necesidades específicas y mejorar tus habilidades en ingeniería de datos."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "SQL",
            "language": "sql",
            "name": "SQL"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

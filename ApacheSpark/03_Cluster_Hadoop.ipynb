{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# [Laboratorio práctico sobre el clúster Hadoop](https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop/ungradedLti/RVAek/hands-on-lab-hadoop-cluster-optional)\n",
                "##   \n",
                "#### ¿Qué es un clúster Hadoop?\n",
                "\n",
                "Un clúster Hadoop es una colección de computadoras, conocidas como nodos, que están interconectadas para realizar cálculos paralelos sobre grandes conjuntos de datos. El nodo maestro es el nodo principal del Sistema de Archivos Distribuido Hadoop (HDFS). Mantiene los metadatos de los archivos en la RAM para un acceso rápido. La configuración real de un clúster Hadoop implica recursos extensivos que no están dentro del alcance de este laboratorio. En este laboratorio, utilizarás Hadoop en contenedores para crear un clúster Hadoop que tendrá:\n",
                "\n",
                "1. Nodo maestro\n",
                "2. Nodo de datos\n",
                "3. Administrador de nodos\n",
                "4. Administrador de recursos\n",
                "5. Servidor de historial de Hadoop\n",
                "\n",
                "## Objetivos\n",
                "\n",
                "- Ejecutar una instancia de clúster Hadoop en contenedores\n",
                "- Crear un archivo en el HDFS y verlo en la interfaz gráfica"
            ],
            "metadata": {
                "azdata_cell_guid": "1ffa2549-6e10-4c8c-b635-d60b15b63748"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Configurar Nodos del Clúster Hadoop Dockerizado\n",
                "\n",
                "1. Inicie una nueva terminal\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/New_terminal.png)\n",
                "\n",
                "2. Clone el repositorio en su entorno theia.\n",
                "\n",
                "```\n",
                "git clone https://github.com/ibm-developer-skills-network/ooxwv-docker_hadoop.git\n",
                "```\n",
                "\n",
                "3. Navega al directorio docker-hadoop para construirlo.\n",
                "\n",
                "```\n",
                "cd ooxwv-docker_hadoop\n",
                "```\n",
                "\n",
                "4. Componer la aplicación docker.\n",
                "\n",
                "```\n",
                "docker-compose up -d\n",
                "```\n",
                "\n",
                "> **Compose** es una herramienta para definir y ejecutar aplicaciones Docker de múltiples contenedores. Utiliza el archivo YAML para configurar los servicios y nos permite crear e iniciar todos los servicios a partir de un solo archivo de configuración.\n",
                "\n",
                "Verás que se crean e inician los cinco contenedores.\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/all_containers.png)\n",
                "\n",
                "5. Ejecuta el namenode como una unidad montada en bash.\n",
                "\n",
                "```\n",
                "docker exec -it namenode /bin/bash\n",
                "```\n",
                "\n",
                "6. Observarás que el aviso cambia como se muestra a continuación.\n",
                "\n",
                "    ![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/root_prompt.png)"
            ],
            "metadata": {
                "azdata_cell_guid": "467a7c34-80bf-45b9-9c8d-2704bf8e3e85"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Explorar el entorno de hadoop\n",
                "\n",
                "Como has aprendido en los videos y lecturas hasta ahora en el curso, un entorno Hadoop se configura editando un conjunto de archivos de configuración:\n",
                "\n",
                "- **hadoop-env.sh** Sirve como un archivo maestro para configurar YARN, HDFS, MapReduce y la configuración de proyectos relacionados con Hadoop.\n",
                "    \n",
                "- **core-site.xml** Define las propiedades principales de HDFS y Hadoop.\n",
                "    \n",
                "- **hdfs-site.xml** Regula la ubicación para almacenar metadatos de nodos, el archivo fsimage y el archivo de registro.\n",
                "    \n",
                "- **mapred-site-xml** Enumera los parámetros para la configuración de MapReduce.\n",
                "    \n",
                "- **yarn-site.xml** Define configuraciones relevantes para YARN. Contiene configuraciones para el Node Manager, Resource Manager, Containers y Application Master.\n",
                "    \n",
                "\n",
                "Para la imagen de docker, estos archivos xml ya han sido configurados. Puedes verlos en el directorio **/opt/hadoop-3.2.1/etc/hadoop/** ejecutando\n",
                "\n",
                "```\n",
                "ls /opt/hadoop-3.2.1/etc/hadoop/*.xml\n",
                "```"
            ],
            "metadata": {
                "azdata_cell_guid": "160928ac-d36d-47c7-84e3-d830d65f1c08"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Crear un archivo en el HDFS\n",
                "\n",
                "1. En el HDFS, crea una estructura de directorios llamada `user/root/input`.\n",
                "\n",
                "```\n",
                "hdfs dfs -mkdir -p /user/root/input\n",
                "```\n",
                "\n",
                "2. Copie todos los archivos de configuración xml de hadoop en el directorio de entrada.\n",
                "\n",
                "```\n",
                "hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /user/root/input\n",
                "```\n",
                "\n",
                "3. Crea un archivo `data.txt` en el directorio actual.\n",
                "\n",
                "```\n",
                "curl https://raw.githubusercontent.com/ibm-developer-skills-network/ooxwv-docker_hadoop/master/SampleMapReduce.txt --output data.txt\n",
                "```\n",
                "\n",
                "4. Copia el archivo `data.txt` en `/user/root`.\n",
                "\n",
                "```\n",
                "hdfs dfs -put data.txt /user/root/\n",
                "```\n",
                "\n",
                "5. Verifica si el archivo ha sido copiado en el HDFS revisando su contenido.\n",
                "\n",
                "```\n",
                "hdfs dfs -cat /user/root/data.txt\n",
                "```"
            ],
            "metadata": {
                "azdata_cell_guid": "7e78b4df-b7e7-4385-915a-e48574487885"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Ver el HDFS\n",
                "\n",
                "1. Haz clic en el botón de abajo o en el botón de Skills Network a la izquierda, se abrirá la “Skills Network Toolbox”. Luego haz clic en Otros y luego en Lanzar Aplicación. Desde allí deberías poder ingresar el número de puerto como `9870` y lanzar.\n",
                "\n",
                " <span class=\"plugin-text\" style=\"scrollbar-color: rgb(69, 74, 77) rgb(32, 35, 36); padding: 0px 0px 0px 2px; margin: 0px; box-sizing: border-box; display: inline;\">Ver HDFS</span>\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/Launch_Application--new_IDE.png)\n",
                "\n",
                "2. Esto abrirá la Interfaz Gráfica de Usuario (GUI) del nodo Hadoop. Haz clic en `Utilities` **\\-\\>** `Broswe the file system` para explorar los archivos.\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/browse_filesystem.png)\n",
                "\n",
                "3. Visualiza los archivos en los directorios que acabas de crear haciendo clic en `user` y luego en `root`.\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Click-User.png)\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/Click-root.png)\n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/browse_directory.png)\n",
                "\n",
                "4. Observa que el tamaño del bloque es de 128 MB aunque el tamaño del archivo es en realidad mucho más pequeño. Esto se debe a que el tamaño de bloque predeterminado utilizado por HDFS es de 128 MB.\n",
                "    \n",
                "5. Puedes hacer clic en el archivo para verificar la información del archivo. Te proporciona información sobre el archivo en términos de número de bytes, id de bloque, etc.\n",
                "    \n",
                "\n",
                "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/images/file_info.png)"
            ],
            "metadata": {
                "azdata_cell_guid": "d0bd20eb-d603-4e84-bd50-e15a71959bca"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### ¡Felicitaciones! Has:\n",
                "\n",
                "- Desplegado Hadoop usando Docker\n",
                "- Creado datos en HDFS y los has visto en la GUI"
            ],
            "metadata": {
                "azdata_cell_guid": "ceef4106-d2fc-49ca-ad18-d49c4e6ec313"
            },
            "attachments": {}
        }
    ]
}